# ğŸ“˜ Week 01 â€“ Introduction & Regression

## ğŸ¯ Goals
- Understand supervised learning and linear regression
- Explore overfitting and underfitting
- Learn how to implement and visualize linear models

---

## ğŸ§  Topics Covered
- Supervised learning: input-output mappings
- Linear regression (least squares, loss function)
- Underfitting vs. overfitting
- Model complexity and generalization

---

## ğŸ“ Math Focus
- Linear algebra: vectors, dot products, matrix multiplication
- Gradient descent intuition and computation
- Cost function minimization

---

## ğŸ’» Coding Tasks
- [ ] Implement linear regression using gradient descent (NumPy)
- [ ] Create synthetic datasets with noise
- [ ] Visualize underfitting vs overfitting by changing model complexity

---

## ğŸŒ Public Deliverables
- [ ] `linear_regression.ipynb` with clear math and visualizations
- [ ] GitHub push with cleaned-up notebook + README section
- [ ] (Optional) Blog post: *"Why Overfitting Happens"*

---

## ğŸ“š Resources
- ğŸ“¼ CS229 Lectures: 1â€“3
- ğŸ“˜ *Understanding ML*: Ch. 2â€“3
- ğŸ“˜ *Deep Learning (Goodfellow)*: Â§5.1â€“5.2
- ğŸ“º 3Blue1Brown: [Linear Algebra playlist](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)
