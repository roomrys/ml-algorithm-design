# 📘 Week 01 – Introduction & Regression

## 🎯 Goals
- Understand supervised learning and linear regression
- Explore overfitting and underfitting
- Learn how to implement and visualize linear models

---

## 🧠 Topics Covered
- Supervised learning: input-output mappings
- Linear regression (least squares, loss function)
- Underfitting vs. overfitting
- Model complexity and generalization

---

## 📐 Math Focus
- Linear algebra: vectors, dot products, matrix multiplication
- Gradient descent intuition and computation
- Cost function minimization

---

## 💻 Coding Tasks
- [ ] Implement linear regression using gradient descent (NumPy)
- [ ] Create synthetic datasets with noise
- [ ] Visualize underfitting vs overfitting by changing model complexity

---

## 🌍 Public Deliverables
- [ ] `linear_regression.ipynb` with clear math and visualizations
- [ ] GitHub push with cleaned-up notebook + README section
- [ ] (Optional) Blog post: *"Why Overfitting Happens"*

---

## 📚 Resources
- 📼 CS229 Lectures: 1–3
- 📘 *Understanding ML*: Ch. 2–3
- 📘 *Deep Learning (Goodfellow)*: §5.1–5.2
- 📺 3Blue1Brown: [Linear Algebra playlist](https://www.youtube.com/playlist?list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr)
