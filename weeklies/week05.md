# ğŸ“˜ Week 05 â€“ Advanced Optimization

## ğŸ¯ Goals
- Deepen understanding of optimization methods used in ML
- Explore variants of stochastic gradient descent
- Understand constrained optimization with Lagrangians

---

## ğŸ§  Topics Covered
- Stochastic gradient descent (SGD), mini-batch
- Momentum, RMSprop, Adam optimizers
- Lagrange multipliers for constrained optimization

---

## ğŸ“ Math Focus
- Convexity and non-convexity
- First- and second-order methods
- Lagrangian duality and Karush-Kuhn-Tucker conditions (conceptually)

---

## ğŸ’» Coding Tasks
- [ ] Implement SGD, momentum, RMSProp, and Adam
- [ ] Compare optimizers on the same problem
- [ ] Visualize loss landscape and learning curves

---

## ğŸŒ Public Deliverables
- [ ] `optimizer_zoo.ipynb` notebook
- [ ] Visual comparison of optimizer behavior (convergence rate, trajectory)
- [ ] (Optional) Blog post: *"What Makes Adam So Popular?"*

---

## ğŸ“š Resources
- ğŸ“˜ *Deep Learning* (Goodfellow): Ch. 8
- ğŸ“˜ *Convex Optimization* (Boyd): Ch. 4â€“5
- ğŸ“¼ CS231n Lecture: Optimization Overview
