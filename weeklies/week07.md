# 📘 Week 07 – Probabilistic Models & EM Algorithm

## 🎯 Goals
- Explore probabilistic classifiers (Naive Bayes)
- Understand latent variable models (GMMs)
- Implement EM algorithm for unsupervised clustering

---

## 🧠 Topics Covered
- Naive Bayes and conditional independence
- Gaussian mixture models (GMM)
- Expectation-Maximization (EM) algorithm

---

## 📐 Math Focus
- Bayes’ Rule
- Log-likelihood of mixtures
- EM steps: E-step and M-step derivation

---

## 💻 Coding Tasks
- [ ] Implement Naive Bayes for classification
- [ ] Implement GMM + EM from scratch
- [ ] Visualize EM steps and convergence

---

## 🌍 Public Deliverables
- [ ] `em_gmm.ipynb`
- [ ] Visual explanation of E-step vs M-step
- [ ] Blog: *"How the EM Algorithm Finds Hidden Structure"*

---

## 📚 Resources
- 📘 *Murphy: Probabilistic ML* – Ch. 9–11
- 📼 CS229 Lectures: 13–14
- 📺 StatQuest: [EM Algorithm](https://www.youtube.com/watch?v=REypj2sy_5U)
