# ğŸ“˜ Week 09 â€“ Deep Learning Primer

## ğŸ¯ Goals
- Understand neural networks and backpropagation
- Implement multilayer perceptron (MLP) from scratch
- Transition to PyTorch

---

## ğŸ§  Topics Covered
- Perceptrons and multilayer networks
- Backpropagation algorithm
- Activation functions and loss functions
- PyTorch basics

---

## ğŸ“ Math Focus
- Chain rule for derivatives
- Forward and backward pass in MLP
- Gradient flow through layers

---

## ğŸ’» Coding Tasks
- [ ] Implement 2-layer MLP from scratch (NumPy)
- [ ] Train and evaluate on toy dataset
- [ ] Re-implement in PyTorch

---

## ğŸŒ Public Deliverables
- [ ] `mlp_from_scratch.ipynb` and `mlp_pytorch.ipynb`
- [ ] Visuals of learned decision boundary
- [ ] Post: *"What Backprop Really Means"*

---

## ğŸ“š Resources
- ğŸ“˜ *Neural Networks & Deep Learning* (Nielsen, free online)
- ğŸ“˜ *Deep Learning* (Goodfellow): Ch. 6
- ğŸ“¼ CS231n: Lectures 1â€“3
