# ğŸ“˜ Week 06 â€“ Tree-Based Methods & Boosting

## ğŸ¯ Goals
- Learn how decision trees work
- Understand ensemble learning: bagging, random forests, boosting
- Implement AdaBoost from scratch

---

## ğŸ§  Topics Covered
- Decision tree structure and splitting criteria (Gini, entropy)
- Bagging and random forests
- AdaBoost algorithm

---

## ğŸ“ Math Focus
- Information gain and entropy
- Weighted loss minimization in boosting
- Bias-variance tradeoff revisited in ensembles

---

## ğŸ’» Coding Tasks
- [ ] Implement basic decision tree (use scikit-learn for comparison)
- [ ] Implement AdaBoost (binary classification)
- [ ] Visualize ensemble decisions

---

## ğŸŒ Public Deliverables
- [ ] `adaboost_from_scratch.ipynb`
- [ ] Comparison between single tree vs ensemble
- [ ] Blog post: *"Boosting: From Intuition to Implementation"*

---

## ğŸ“š Resources
- ğŸ“˜ *Understanding ML*: Ch. 10, 30
- ğŸ“¼ StatQuest: [AdaBoost](https://www.youtube.com/watch?v=LsK-xG1cLYA)
- ğŸ“˜ *Elements of Statistical Learning*: Ch. 10
