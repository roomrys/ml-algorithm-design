# ğŸ“˜ Week 02 â€“ Classification & Optimization

## ğŸ¯ Goals
- Learn binary classification and logistic regression
- Understand gradient vs Newton optimization
- Practice implementing both methods

---

## ğŸ§  Topics Covered
- Logistic regression and sigmoid function
- Cross-entropy loss
- Gradient descent vs Newtonâ€™s method
- Multiclass classification (optional)

---

## ğŸ“ Math Focus
- Gradients and Hessians
- Log-likelihood maximization
- Convexity of logistic regression loss

---

## ğŸ’» Coding Tasks
- [ ] Implement logistic regression using gradient descent
- [ ] Implement Newtonâ€™s method for optimization
- [ ] Compare convergence rates of the two methods

---

## ğŸŒ Public Deliverables
- [ ] `logistic_regression.ipynb` (binary classification)
- [ ] Visual comparison of gradient vs Newton steps
- [ ] (Optional) Blog post: *"Why Newtonâ€™s Method is Faster"*

---

## ğŸ“š Resources
- ğŸ“¼ CS229 Lectures: 4â€“6
- ğŸ“˜ *Understanding ML*: Ch. 4â€“5
- ğŸ“˜ *Convex Optimization (Boyd)*: Ch. 9
- ğŸ“º StatQuest: [Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8)
