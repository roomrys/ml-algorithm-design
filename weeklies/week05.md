# 📘 Week 05 – Advanced Optimization

## 🎯 Goals
- Deepen understanding of optimization methods used in ML
- Explore variants of stochastic gradient descent
- Understand constrained optimization with Lagrangians

---

## 🧠 Topics Covered
- Stochastic gradient descent (SGD), mini-batch
- Momentum, RMSprop, Adam optimizers
- Lagrange multipliers for constrained optimization

---

## 📐 Math Focus
- Convexity and non-convexity
- First- and second-order methods
- Lagrangian duality and Karush-Kuhn-Tucker conditions (conceptually)

---

## 💻 Coding Tasks
- [ ] Implement SGD, momentum, RMSProp, and Adam
- [ ] Compare optimizers on the same problem
- [ ] Visualize loss landscape and learning curves

---

## 🌍 Public Deliverables
- [ ] `optimizer_zoo.ipynb` notebook
- [ ] Visual comparison of optimizer behavior (convergence rate, trajectory)
- [ ] (Optional) Blog post: *"What Makes Adam So Popular?"*

---

## 📚 Resources
- 📘 *Deep Learning* (Goodfellow): Ch. 8
- 📘 *Convex Optimization* (Boyd): Ch. 4–5
- 📼 CS231n Lecture: Optimization Overview
