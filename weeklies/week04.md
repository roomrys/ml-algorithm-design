# 📘 Week 04 – Theory & Unsupervised Learning

## 🎯 Goals
- Learn about generalization theory and VC-dimension
- Explore regularization as a Bayesian prior
- Implement unsupervised learning algorithms (k-means, PCA)

---

## 🧠 Topics Covered
- Generalization bounds, VC-dimension (intro)
- Bayesian perspective on L2 regularization
- K-means clustering
- Principal Component Analysis (PCA)

---

## 📐 Math Focus
- VC-dimension intuition
- Eigenvectors and eigenvalues
- L2 regularization as MAP estimation

---

## 💻 Coding Tasks
- [ ] Implement K-means clustering
- [ ] Implement PCA using eigen decomposition
- [ ] Visualize dimensionality reduction results

---

## 🌍 Public Deliverables
- [ ] `unsupervised.ipynb`: k-means + PCA
- [ ] “L2 regularization = Gaussian prior” math explanation
- [ ] (Optional) Blog post: *"How Unsupervised Learning Finds Structure"*

---

## 📚 Resources
- 📼 CS229 Lectures: 10–12
- 📘 *Understanding ML*: Ch. 4–5, 19
- 📘 *Murphy's ML Probabilistic Perspective*: Ch. 5
- 📺 3Blue1Brown: [Eigenvectors](https://www.youtube.com/watch?v=PFDu9oVAE-g)
