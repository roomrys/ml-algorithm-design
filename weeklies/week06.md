# 📘 Week 06 – Tree-Based Methods & Boosting

## 🎯 Goals
- Learn how decision trees work
- Understand ensemble learning: bagging, random forests, boosting
- Implement AdaBoost from scratch

---

## 🧠 Topics Covered
- Decision tree structure and splitting criteria (Gini, entropy)
- Bagging and random forests
- AdaBoost algorithm

---

## 📐 Math Focus
- Information gain and entropy
- Weighted loss minimization in boosting
- Bias-variance tradeoff revisited in ensembles

---

## 💻 Coding Tasks
- [ ] Implement basic decision tree (use scikit-learn for comparison)
- [ ] Implement AdaBoost (binary classification)
- [ ] Visualize ensemble decisions

---

## 🌍 Public Deliverables
- [ ] `adaboost_from_scratch.ipynb`
- [ ] Comparison between single tree vs ensemble
- [ ] Blog post: *"Boosting: From Intuition to Implementation"*

---

## 📚 Resources
- 📘 *Understanding ML*: Ch. 10, 30
- 📼 StatQuest: [AdaBoost](https://www.youtube.com/watch?v=LsK-xG1cLYA)
- 📘 *Elements of Statistical Learning*: Ch. 10
