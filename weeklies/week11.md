# 📘 Week 11 – Generalization & Implicit Bias

## 🎯 Goals
- Explore how and why ML models generalize
- Understand implicit bias in optimization (e.g. SGD dynamics)
- Revisit double descent and modern generalization puzzles

---

## 🧠 Topics Covered
- Generalization gap: training vs test error
- Implicit bias of gradient descent (why SGD prefers simpler solutions)
- Double descent and the interpolation threshold

---

## 📐 Math Focus
- Generalization bounds
- SGD dynamics in overparameterized models
- Intuition from PAC-learning and norm minimization

---

## 💻 Coding Tasks
- [ ] Simulate training dynamics of SGD on overparameterized models
- [ ] Plot loss curves and generalization behavior

---

## 🌍 Public Deliverables
- [ ] `generalization_dynamics.ipynb`
- [ ] Visual notebook showing generalization gap behavior
- [ ] (Optional) Blog post: *"Why SGD Generalizes Surprisingly Well"*

---

## 📚 Resources
- 📘 Zhang et al. (2016): *Understanding Deep Learning Requires Rethinking Generalization*
- 📘 Neyshabur et al. (2017): *Implicit Regularization in Deep Learning*
- 📼 MIT 6.S897: Generalization in Deep Learning (2020)
