# ğŸ“˜ Week 07 â€“ Probabilistic Models & EM Algorithm

## ğŸ¯ Goals
- Explore probabilistic classifiers (Naive Bayes)
- Understand latent variable models (GMMs)
- Implement EM algorithm for unsupervised clustering

---

## ğŸ§  Topics Covered
- Naive Bayes and conditional independence
- Gaussian mixture models (GMM)
- Expectation-Maximization (EM) algorithm

---

## ğŸ“ Math Focus
- Bayesâ€™ Rule
- Log-likelihood of mixtures
- EM steps: E-step and M-step derivation

---

## ğŸ’» Coding Tasks
- [ ] Implement Naive Bayes for classification
- [ ] Implement GMM + EM from scratch
- [ ] Visualize EM steps and convergence

---

## ğŸŒ Public Deliverables
- [ ] `em_gmm.ipynb`
- [ ] Visual explanation of E-step vs M-step
- [ ] Blog: *"How the EM Algorithm Finds Hidden Structure"*

---

## ğŸ“š Resources
- ğŸ“˜ *Murphy: Probabilistic ML* â€“ Ch. 9â€“11
- ğŸ“¼ CS229 Lectures: 13â€“14
- ğŸ“º StatQuest: [EM Algorithm](https://www.youtube.com/watch?v=REypj2sy_5U)
