# 📘 Week 02 – Classification & Optimization

## 🎯 Goals
- Learn binary classification and logistic regression
- Understand gradient vs Newton optimization
- Practice implementing both methods

---

## 🧠 Topics Covered
- Logistic regression and sigmoid function
- Cross-entropy loss
- Gradient descent vs Newton’s method
- Multiclass classification (optional)

---

## 📐 Math Focus
- Gradients and Hessians
- Log-likelihood maximization
- Convexity of logistic regression loss

---

## 💻 Coding Tasks
- [ ] Implement logistic regression using gradient descent
- [ ] Implement Newton’s method for optimization
- [ ] Compare convergence rates of the two methods

---

## 🌍 Public Deliverables
- [ ] `logistic_regression.ipynb` (binary classification)
- [ ] Visual comparison of gradient vs Newton steps
- [ ] (Optional) Blog post: *"Why Newton’s Method is Faster"*

---

## 📚 Resources
- 📼 CS229 Lectures: 4–6
- 📘 *Understanding ML*: Ch. 4–5
- 📘 *Convex Optimization (Boyd)*: Ch. 9
- 📺 StatQuest: [Logistic Regression](https://www.youtube.com/watch?v=yIYKR4sgzI8)
