# üß† Machine Learning Algorithm Design Syllabus

This is a full-time self-study syllabus focused on building a strong foundation in machine learning with an emphasis on algorithm design, theory, and implementation. Estimated timeline: 3‚Äì4 months (30‚Äì40 hrs/week).

---

## üìÖ Weekly Schedule Overview

| Week | Focus Area                                | Key Topics                                                                 | Deliverables                                |
|------|--------------------------------------------|----------------------------------------------------------------------------|---------------------------------------------|
| 1    | Intro to ML & Regression                   | Supervised learning, linear regression, overfitting                        | Linear Regression notebook + GitHub repo    |
| 2    | Classification & Optimization              | Logistic regression, gradient descent, Newton‚Äôs method                     | Logistic Regression (scratch) + writeup     |
| 3    | SVMs, Kernels, and Bias-Variance           | SVM, hinge loss, kernel trick, bias-variance tradeoff                      | SVM from scratch + kernel demos             |
| 4    | Theory & Clustering                        | VC-dimension, generalization, k-means, PCA                                 | K-means & PCA + ‚ÄúWhy L2 = Prior‚Äù notebook   |
| 5    | Advanced Optimization                      | SGD variants, momentum, Adam, constrained optimization                     | Optimizer zoo notebook                      |
| 6    | Tree-Based Methods                         | Decision trees, random forests, boosting (AdaBoost)                        | AdaBoost scratch impl + visualization       |
| 7    | Probabilistic Models                       | Naive Bayes, EM algorithm, Gaussian mixtures                               | EM/GMM notebook + insights                  |
| 8    | Paper Reproduction I                       | Reproduce 1‚Äì2 foundational ML papers (e.g. AdaBoost, SVM)                  | Code + blog summary for 1 paper             |
| 9    | Deep Learning Primer                       | MLPs, backpropagation, intro to PyTorch/TensorFlow                         | MLP scratch impl + torch version            |
| 10   | Paper Reproduction II                      | Choose a recent paper in optimization or theory                            | Code, visuals, written explanation          |
| 11   | Theory: Generalization & Implicit Bias     | SGD bias, double descent, generalization gaps                              | Notebook + diagram-led explanation          |
| 12   | Final Project Prep                         | Design & scope one larger original project or deep reproduction            | Project outline + repo structure            |
| 13‚Äì14| Capstone Project (Implementation)          | Execute project: novel optimization algo, paper reproduction, or toolkit   | Codebase + README + math doc                |
| 15‚Äì16| Polish, Apply, and Share                   | Resume, GitHub polish, blog posts, apply to research jobs/fellowships      | Public blog, resume, job applications       |

---

## üîß Tools Used Throughout

- Languages/Libraries: Python, NumPy, Matplotlib, Scikit-learn, PyTorch (later)
- Platforms: GitHub, Jupyter, optionally Streamlit/Gradio for demos
- Optional blogging: GitHub Pages, Medium, or personal blog

---

## üß≠ Outcome Goals

- Deep understanding of ML theory & algorithm design
- 10+ original notebooks and scratch implementations
- 1‚Äì2 full paper reproductions
- 1 polished capstone project
- Public GitHub repo + writing trail
