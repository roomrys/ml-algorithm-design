# ğŸ“˜ Week 11 â€“ Generalization & Implicit Bias

## ğŸ¯ Goals
- Explore how and why ML models generalize
- Understand implicit bias in optimization (e.g. SGD dynamics)
- Revisit double descent and modern generalization puzzles

---

## ğŸ§  Topics Covered
- Generalization gap: training vs test error
- Implicit bias of gradient descent (why SGD prefers simpler solutions)
- Double descent and the interpolation threshold

---

## ğŸ“ Math Focus
- Generalization bounds
- SGD dynamics in overparameterized models
- Intuition from PAC-learning and norm minimization

---

## ğŸ’» Coding Tasks
- [ ] Simulate training dynamics of SGD on overparameterized models
- [ ] Plot loss curves and generalization behavior

---

## ğŸŒ Public Deliverables
- [ ] `generalization_dynamics.ipynb`
- [ ] Visual notebook showing generalization gap behavior
- [ ] (Optional) Blog post: *"Why SGD Generalizes Surprisingly Well"*

---

## ğŸ“š Resources
- ğŸ“˜ Zhang et al. (2016): *Understanding Deep Learning Requires Rethinking Generalization*
- ğŸ“˜ Neyshabur et al. (2017): *Implicit Regularization in Deep Learning*
- ğŸ“¼ MIT 6.S897: Generalization in Deep Learning (2020)
